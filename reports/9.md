# Lab Report 9

517030910384 徐尚宁

## Introduction

In this lab report, we first briefly describe the installation process of Hadoop, and then demonstrate the MapReduce algorithm with the examples provided in the exercises, most notably among them being the computation of PageRank.

## Environment Setup

The installation process would be tedious if Hadoop is to be installed manually. Fortunately, in AUR (Arch User Repository) of Arch Linux an [automatic packaging and installation script](https://aur.archlinux.org/packages/hadoop/), or in Arch terminology, a `PKGBUILD`, is available. We simply run

```sh
$ makepkg -si
```

in the directory that contains the script to install the latest version (3.1.1) of Hadoop.

Since Hadoop is running as the user `hadoop`, we need to set up passphraseless SSH. First we change the password of the user `hadoop` with root privilege and then change the user to `hadoop` with `su`:

```sh
$ sudo passwd hadoop
New password: 
Retype new password: 
passwd: password updated successfully
$ su hadoop
Password:
```

An SSH key pair is generated without passphrase and added to trusted login keys:

```sh
$ ssh-keygen -t rsa -P ''
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

All configuration files reside in `/etc/hadoop/`, which happens to be the home folder of the user `hadoop`. We add a short configuration file `core-site.xml` to the folder:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

As of this stage, we are ready to start Hadoop:

```sh
sudo systemctl start hadoop-{namenode,datanode,secondarynamenode}
```

### A Note on Version

Version 2 and 3 exhibit almost no difference in running the exercises. A majority of notable changes concern the command syntax. For example, `hadoop fs` is now invoked with `hdfs dfs`, and there is a dedicated command for Hadoop streaming: `mapred streaming`.

## Running Examples

We invoke the built-in example for calculating π:

```sh
hadoop jar /usr/lib/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar pi <nMaps> <nSamples>
```

We fill the first table by running the command above.

Number of maps | Number of samples | Time(s) | π
--- | --- | --- | ---
2 | 10 | 1.684 | 3.80000000000000000000
5 | 10 | 1.701 | 3.28000000000000000000
10 | 10 | 1.638 | 3.20000000000000000000
2 | 100 | 1.624 | 3.12000000000000000000
10 | 100 | 1.731 | 3.14800000000000000000

As for the second exercise, a simple Python script is written to aide in finding the parameters. Essentially, the script utilises the `subprocess` module to execute commands within Python, captures the command output and extracts results. In this way, we not only don't need to deal with shell scripts dreadful syntax, but also have access to familiar string processing tools in Python. Each iteration of the loop sees the parameter `nSamples`, denoted by `power` in the code, multiply itself by 10 and fed into the Hadoop example. Results are written in CSV format.

```python
"""Increase the number of samples by a decade until a desirable precision in
computing π is achieved"""

import subprocess

f = open('mini_ex.csv', 'w')

f.write('Number of samples,Time,Value of approximation\n')
for i in range(20):
    power = 10 ** (i + 1)   # used as number of samples
    cp = subprocess.run(
        (
            'hadoop', 'jar',
            '/usr/lib/hadoop-3.1.1/share/hadoop/mapreduce/'
            'hadoop-mapreduce-examples-3.1.1.jar',
            'pi', '5', str(power)
        ),
        capture_output=True,
        encoding='UTF-8'
    )
    # select two lines to extract the time and calculated value
    results = cp.stdout.split('\n')[-3:-1]
    # The following two comment lines are sample output:
    # Job Finished in 1.653 seconds
    # Estimated value of Pi is 3.80000000000000000000
    pi_str = results[1].split(' ')[-1]
    pi = float(pi_str)
    # split the first line in white space and extract the third element
    # i.e. the running time
    f.write(str(power) + ',' + results[0].split(' ')[3] + ',' + pi_str + '\n')
    print(power)

    if abs(pi - 3.14159) < 0.00001:
        break

f.close()
```

The second table gives answers to the second exercise:

Number of samples | Time | Value of approximation
--- | --- | ---
10 | 1.685 | 3.28000000000000000000
100 | 1.677 | 3.16000000000000000000
1000 | 1.711 | 3.14160000000000000000
10000 | 1.643 | 3.14248000000000000000
100000 | 1.62 | 3.14159200000000000000
