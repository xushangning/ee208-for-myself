# Lab Report 9

517030910384 徐尚宁

## Introduction

In this lab report, we first briefly describe the installation process of Hadoop, and then demonstrate the MapReduce algorithm with the examples provided in the exercises, most notably among them being the computation of PageRank.

## Environment Setup

The installation process would be tedious if Hadoop is to be installed manually. Fortunately, in AUR (Arch User Repository) of Arch Linux an [automatic packaging and installation script](https://aur.archlinux.org/packages/hadoop/), or in Arch terminology, a `PKGBUILD`, is available. We simply run

```sh
$ makepkg -si
```

in the directory that contains the script to install the latest version (3.1.1) of Hadoop.

Since Hadoop is running as the user `hadoop`, we need to set up passphraseless SSH. First we change the password of the user `hadoop` with root privilege and then change the user to `hadoop` with `su`:

```sh
$ sudo passwd hadoop
New password: 
Retype new password: 
passwd: password updated successfully
$ su hadoop
Password:
```

An SSH key pair is generated without passphrase and added to trusted login keys:

```sh
$ ssh-keygen -t rsa -P ''
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

All configuration files reside in `/etc/hadoop/`, which happens to be the home folder of the user `hadoop`. We add a short configuration file `core-site.xml` to the folder:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

As of this stage, we are ready to start Hadoop:

```sh
sudo systemctl start hadoop-{namenode,datanode,secondarynamenode}
```

### A Note on Version

Version 2 and 3 exhibit almost no difference in running the exercises. A majority of notable changes concern the command syntax. For example, `hadoop fs` is now invoked with `hdfs dfs`, and there is a dedicated command for Hadoop streaming: `mapred streaming`.

## Running Examples

We invoke the built-in example for calculating π:

```sh
hadoop jar /usr/lib/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar pi <nMaps> <nSamples>
```

We fill the first table by running the command above.

Number of maps | Number of samples | Time(s) | π
--- | --- | --- | ---
2 | 10 | 1.684 | 3.80000000000000000000
5 | 10 | 1.701 | 3.28000000000000000000
10 | 10 | 1.638 | 3.20000000000000000000
2 | 100 | 1.624 | 3.12000000000000000000
10 | 100 | 1.731 | 3.14800000000000000000

As for the second exercise, a simple Python script is written to aide in finding the parameters. Essentially, the script utilises the `subprocess` module to execute commands within Python, captures the command output and extracts results. In this way, we not only don't need to deal with shell scripts dreadful syntax, but also have access to familiar string processing tools in Python. Each iteration of the loop sees the parameter `nSamples`, denoted by `power` in the code, multiply itself by 10 and fed into the Hadoop example. Results are written in CSV format.

```python
"""Increase the number of samples by a decade until a desirable precision in
computing π is achieved"""

import subprocess

f = open('mini_ex.csv', 'w')

f.write('Number of samples,Time,Value of approximation\n')
for i in range(20):
    power = 10 ** (i + 1)   # used as number of samples
    cp = subprocess.run(
        (
            'hadoop', 'jar',
            '/usr/lib/hadoop-3.1.1/share/hadoop/mapreduce/'
            'hadoop-mapreduce-examples-3.1.1.jar',
            'pi', '5', str(power)
        ),
        capture_output=True,
        encoding='UTF-8'
    )
    # select two lines to extract the time and calculated value
    results = cp.stdout.split('\n')[-3:-1]
    # The following two comment lines are sample output:
    # Job Finished in 1.653 seconds
    # Estimated value of Pi is 3.80000000000000000000
    pi_str = results[1].split(' ')[-1]
    pi = float(pi_str)
    # split the first line in white space and extract the third element
    # i.e. the running time
    f.write(str(power) + ',' + results[0].split(' ')[3] + ',' + pi_str + '\n')
    print(power)

    if abs(pi - 3.14159) < 0.00001:
        break

f.close()
```

The second table gives answers to the second exercise:

Number of samples | Time | Value of approximation
--- | --- | ---
10 | 1.685 | 3.28000000000000000000
100 | 1.677 | 3.16000000000000000000
1000 | 1.711 | 3.14160000000000000000
10000 | 1.643 | 3.14248000000000000000
100000 | 1.62 | 3.14159200000000000000

## Average Length of Words

In this exercise, we view the mapper and the reducer more as a two-step operation instead of dealing with key-value pairs. We calculate the average length in the following way:

1. `mapper.py` initialises two arrays of length 26: one for storing the total length of words beginning with each letter and the other for counting the number of words beginning with each letter. If fills the arrays by sifting through the input file and, after reading the file, concatenate two arrays and output the result.

```python
#!/usr/bin/python

import sys
import numpy as np
from string import ascii_letters

word_total_length = np.zeros(26, np.uint64)
word_count = np.zeros(26, np.uint32)

for line in sys.stdin:
    words = line.rstrip().split(' ')
    for w in words:
        if len(w) and w[0] in ascii_letters:
            index = ord(w[0].lower()) - 97
            try:
                word_total_length[index] += len(w)
            except IndexError as e:
                print(w[0])
                raise e
            word_count[index] += 1

for i in word_total_length:
    print(i, end=' ')
for i in word_count[:25]:       # leave out the last white space
    print(i, end=' ')
print(word_count[25])           # and replace it with a line feed
```

2. `reducer.py` only needs to divide each input line into two arrays and add together the total word length and word count, as `mapper.py` has done the heavy-lifting work.

```python
#!/usr/bin/python

import sys
import numpy as np

word_total_length = np.zeros(26, np.uint64)
word_count = np.zeros(26, np.uint32)

for line in sys.stdin:
    data = [int(n) for n in line.rstrip().split(' ')]
    word_total_length += np.array(data[:26], dtype=np.uint64)
    word_count += np.array(data[26:], dtype=np.uint32)

for i in range(26):
    if word_count[i]:
        print(chr(i + 97), word_total_length[i] / word_count[i])
```

## PageRank

### The Single-Thread Version

The most straight forward version of PageRank is implemented with matrix multiplication. We construct an adjacency matrix of the graph from input and then derive the transition probability matrix. The PageRank algorithm states that the PageRank of a webpage will be evenly distributed among webpages that it links to. Each iteration, where for each node its PageRank will be recalculated, can be expressed as the state vector multiplies the transition probability matrix.
It is easy to take factors such as teleport probability α into consideration.

We can derive the matrix in the following steps:

1. For each row in the adjacency matrix, if all entries are zeroes, replace the zeroes with 1 / N, where N is the number of vertices in the graph
2. Otherwise, replace each entry of value 1 with (1 - α) * r / n + α / n, where r is the PageRank of the vertex corresponding to the row, and n is the number of outgoing vertices.

If the transition probability matrix is known, we can repeatedly apply it to the state vector until the state vector converges. The algorithm is implemented in the following code.

```python
import numpy as np


def construct_adj_matrix():
    """
    Construct an adjacency matrix from input in the form
        <id>\t<next_id1> <next_id2> ...
    :return: numpy.ndarray an adjacency matrix
    """
    n = int(input())    # the dimension of the adjacency matrix
    adj_m = np.zeros((n, n), np.int8)
    for _ in range(n):
        line = input()
        vertex_id, line = line.split('\t', 1)
        vertex_id = int(vertex_id) - 1
        for next_id in (int(x) - 1 for x in line.split(' ')):
            adj_m[vertex_id][next_id] = 1
    return adj_m


def pagerank(alpha, adj_m, n_iter):
    """
    Calculate PageRank for a graph specified in an n * n adjacency matrix.
    :param alpha: float the teleport probability
    :param adj_m: numpy.ndarray an n * n adjacency matrix
    :param n_iter: int the number of iterations
    :return: numpy.ndarray an n * 1 vector
    """
    n = adj_m.shape[0]
    tpm = np.array(adj_m, np.float64)     # transition probability matrix
    result = np.zeros(n)
    result[0] = 1

    # construct the transition probability matrix for PageRank
    for i in range(len(tpm)):
        n_nonzero_entries = np.count_nonzero(tpm[i])
        if n_nonzero_entries:
            tpm[i] *= (1 - alpha) / n_nonzero_entries
            tpm[i] += alpha / n
        else:
            tpm[i] = np.full(n, 1 / n)

    for i in range(n_iter):
        result = result @ tpm
        print(result)
    return result


if __name__ == '__main__':
    adj_m = construct_adj_matrix()
    n_iter = int(input())   # the number of iterations to run PageRank
    pagerank(0.5, adj_m, n_iter)
```

### Map & Reduce

`mapper.py` takes an input of the format

```
<vertex_id>\t<pagerank> <out_vertex_id> ...
```

and converts it to two kinds of output:

```
1\t0.0002
|1 2 3 4 5
```

The first kind of the output represents a pagerank received by vertex 1, while the second kind, starting with a vertical bar, denote the outgoing vertices of vertex 1, namely vertex 2, 3, 4, 5.

Note that in the example implementation given in the slide for reference, PageRank contributed by teleporting is somehow missing. That is, when the web surfer starts from a vertex in the internet, they has probability (1 - α) to click the outlinks and α to teleport to a random webpage by typing in the address bar, thus mandating that each vertex in the internet will gain some PageRank from that vertex.

`reducer.py` sums all PageRank for a vertex together and concatenates the PageRank and its outgoing vertices in the same manner as the input.

To test our mapper and reducer, we randomly generated a graph with 1000 vertices and even more edges with `generate_graph.py`. The input is split into four files under the directory `input`, each containing 250 vertices. 20 iterations are run on the graph to obtain near convergence. The reducer output at each stage is stored under `output` and a log of Hadoop running 20 iterations is available in `output/hadoop.log`.
