# Lab Report 5

517030910384 徐尚宁

## Introduction

In this lab report, we present a simple search engine, built on top of PyLucene. The search engine is capable of searching both webpages and images by title or description. Search results can be restricted to a particular domain by specifying `site` in the query.

The report is organized as follows: we first introduce the environment setup of PyLucene, and then present a rewritten version of the web crawler. After gathering webpages with the crawler, we build an index and search over the index for results. The crawler, the indexing and searching procedures are slightly modified to cater to the need of an image search engine, whose data are sourced from [Duokan](https://www.duokan.com).

## Environment Setup

The following list is the base environment on which PyLucene is installed:

- Arch Linux
- Python 3.7.1
- Java OpenJDK 10.0.2

The first step is to install JCC, a compile-time dependency of PyLucene. However, a simple `pip install jcc` won't work because the PyPI doesn't have binary files for JCC and auto-building fails. On [JCC PyPi](https://pypi.org/project/JCC/):

> JCC’s setup.py file needs to be edited before building JCC to specify the location of the Java Runtime Environment’s header files and libraries.

A close inspection reveals that `setup.py` should be modified in several places:

The following code configure the JDK directory for various platforms.

```python
JDK = {
    'darwin': JAVAHOME or JAVAFRAMEWORKS,
    'ipod': '/usr/include/gcc',
    'linux': '/usr/lib/jvm/java-8-oracle',  # <- Linux here
    'sunos5': '/usr/jdk/instances/jdk1.6.0',
    'win32': JAVAHOME,
    'mingw32': JAVAHOME,
    'freebsd7': '/usr/local/diablo-jdk1.6.0'
}
```

It assumes Oracle JDK 8 for Linux, which is false in our case and is changed to

```python
JDK = {
    ...
    'linux': '/usr/lib/jvm/java-10-openjdk'
    ...
}
```

Also note the dict `LFLAGS`, which are used by GCC to link libraries:

```python
LFLAGS = {
    'darwin/frameworks': ['-framework', 'JavaVM', '-mmacosx-version-min=10.9']
    'darwin/home': ['-L%(darwin)s/jre/lib' %(JDK), '-ljava',
                    '-L%(darwin)s/jre/lib/server' %(JDK), '-ljvm',
                    '-Wl,-rpath', '-Wl,%(darwin)s/jre/lib' %(JDK),
                    '-Wl,-rpath', '-Wl,%(darwin)s/jre/lib/server' %(JDK),
                    '-mmacosx-version-min=10.9'],
    'ipod': ['-ljvm', '-lpython%s.%s' %(sys.version_info[0:2]),
             '-L/usr/lib/gcc/arm-apple-darwin9/4.0.1'],
    'linux/i386': ['-L%(linux)s/jre/lib/i386' %(JDK), '-ljava',
                    '-L%(linux)s/jre/lib/i386/client' %(JDK), '-ljvm',
                    '-Wl,-rpath=%(linux)s/jre/lib/i386:%(linux)s/jre/lib/i386/client' %(JDK)],
    'linux/i686': ['-L%(linux)s/jre/lib/i386' %(JDK), '-ljava',
                    '-L%(linux)s/jre/lib/i386/client' %(JDK), '-ljvm',
                    '-Wl,-rpath=%(linux)s/jre/lib/i386:%(linux)s/jre/lib/i386/client' %(JDK)],
    'linux/x86_64': ['-L%(linux)s/jre/lib/amd64' %(JDK), '-ljava',
                      '-L%(linux)s/jre/lib/amd64/server' %(JDK), '-ljvm',
                      '-Wl,-rpath=%(linux)s/jre/lib/amd64:%(linux)s/jre/lib/amd64/server' %(JDK)],
    'sunos5': ['-L%(sunos5)s/jre/lib/i386' %(JDK), '-ljava',
               '-L%(sunos5)s/jre/lib/i386/client' %(JDK), '-ljvm',
               '-R%(sunos5)s/jre/lib/i386:%(sunos5)s/jre/lib/i386/client' %(JDK)],
    'win32': ['/DLL', '/LIBPATH:%(win32)s/lib' %(JDK), 'Ws2_32.lib', 'jvm.lib'],
    'mingw32': ['-L%(mingw32)s/lib' %(JDK), '-ljvm'],
    'freebsd7': ['-L%(freebsd7)s/jre/lib/i386' %(JDK), '-ljava', '-lverify',
                 '-L%(freebsd7)s/jre/lib/i386/client' %(JDK), '-ljvm',
                 '-Wl,-rpath=%(freebsd7)s/jre/lib/i386:%(freebsd7)s/jre/lib/i386/client' %(JDK)],
}
```

In the key `linux/x86_64`, `setup.py` assumes that library files are located under `JDK['linux'] + '/jre/lib/amd64` i.e. `/usr/lib/jvm/java-10-openjdk/jre/lib/amd64/'`, where `JDK` refers to the `dict` just mentioned. For OpenJDK 10, the library files are moved to `JDK['linux'] + '/lib/'` i.e. just `/usr/lib/jvm/java-10-openjdk/lib/`.

After modifications to `LFLAGS`, the dict is

```python
LFLAGS = {
    ...
    'linux/x86_64': ['-L%(linux)s/lib' %(JDK), '-ljava',
                      '-L%(linux)s/lib/server' %(JDK), '-ljvm',
                      '-Wl,-rpath=%(linux)s/jre/lib:%(linux)s/jre/lib/server' %(JDK)],
    ...
}
```

There is one more place to configure. PyLucene requires that JCC supports compiling in "shared mode". During compilation of JCC, shared mode support is only enabled if `setup.py` detects a "modern setuptools" on the OS:

```python
try:
    from pkg_resources import SetuptoolsVersion
    with_modern_setuptools = True
except ImportError:
    try:
        from pkg_resources.extern.packaging.version import Version
        with_modern_setuptools = True
    except ImportError:
        with_modern_setuptools = False
```

Due to customizations of Setuptools at Arch Linux part, the line

```python
from pkg_resources.extern.packaging.version import Version
```

fails because `packaging.version` is available as a separate module instead of a sub-module of `pkg_resources`, thus throwing an `ImportError`. The fix is simple:

```python
from packaging.version import Version
```

Now we are done with `setup.py`, it is time to run `setup.py build` and `setup.py install`.

Next step is to install PyLucene. Uncomment the following lines in `Makefile`:

```makefile
# Linux     (Debian Jessie 64-bit, Python 3.4.2, Oracle Java 1.8                
# Be sure to also set JDK['linux'] in jcc's setup.py to the JAVA_HOME value     
# used below for ANT (and rebuild jcc after changing it).                       
#PREFIX_PYTHON=/usr
#ANT=JAVA_HOME=/usr/lib/jvm/java-8-oracle /usr/bin/ant
#PYTHON=$(PREFIX_PYTHON)/bin/python3
#JCC=$(PYTHON) -m jcc --shared
#NUM_FILES=8
```

and make changes to `ANT=JAVA_HOME=` so that it points to `/usr/lib/jvm/java-10-openjdk`. Before compiling, make sure that Ant is installed.

The final step is running `make` and `make install`. Verify the installation with:

```python
>>> import lucene
>>> lucene.initVM()
```

## Rewriting the Crawler

### The Problem

We identify several important issues with the original crawler:

1. Difficulty of debugging. It is impossible to identify each thread with a unique name, so it is difficult to tell which thread has ended due to an (uncaught) exception or simply, the webpage is crawled by which thread.
2. Non-existent encoding handling.
3. Detection of the content type of response is missing. Threads are often occupied by a lengthy response e.g. a PDF file download.
4. Unclear execution logic. The multi-thread crawler in `parallel_crawler.py` is built on `crawler.py`, which is based on the `crawle.py` included in the lab materials. The single-threaded `crawler.py` is no longer used and serves only as a base for `parallel_crawler.py`. In `parallel_crawler.py` you can see:
    - dangling functions imported from `crawler.py`
    - a function definition `crawling` inserted between statements. This is deliberate to allow `crawling()` to access global variables shared by all threads.

These problems are highlighted in `parallel_crawler.py` below.

```python
import threading
from queue import Queue

from bloomfilter import BloomFilter
import crawler
from GeneralHashFunctions import *

seed = 'https://www.sjtu.edu.cn'
q = Queue()
max_page = 2000
count_pages = 0
graph = {}

BIT_ARRAY_SIZE = 16384
my_filter = BloomFilter(BIT_ARRAY_SIZE, hash, RSHash, JSHash, SDBMHash, FNVHash)

N_THREADS = 4
var_lock = threading.Lock()     # <- main execution


def crawling():                 # <- function definition
    global count_pages  # <- access global variables
    while count_pages < max_page:   # <- access global variables
        print(count_pages, end=' ')
        url = q.get()
        if not my_filter.query(url):
            print(url)
            content = crawler.get_page(url) # <- imported from crawler
            if len(content):
                my_filter.set(url)
                crawler.add_page_to_folder(url, content)
                outlinks = crawler.get_all_links(content, url)
                var_lock.acquire()  # ready to change the global environment
                count_pages += 1
                if len(outlinks):
                    graph[url] = outlinks
                    for link in outlinks:
                        q.put(link)
                var_lock.release()
        q.task_done()


q.put(seed)                     # <- main execution again
thread_pool = []
for _ in range(N_THREADS):
    t = threading.Thread(target=crawling)
    t.start()
    thread_pool.append(t)
for t in thread_pool:
    t.join()

my_filter.print_stats()
print("Number of Crawled Pages:", count_pages)
```

### New `parallel_crawler.py`

There are some distinct features for the new `parallel_crawler.py`:

- The working function `crawling()` becomes a thing of the past. We define a `CrawlerThread` class that inherits `threading.Thread` and overwrite `threading.Thread.run()`.
- Variables shared among threads are now class variables of `CrawlerThread`. Examples of shared variables are the Bloom filter, the job queue and `count_pages`.
- Each thread is identified by their name of the thread.
- Detection of encoding and the content type of a web page.

The new `parallel_crawler.py` is reproduced below with extensive comments.

```python
import os
import threading
from queue import Queue
import urllib.parse

import requests
from bs4 import BeautifulSoup

from GeneralHashFunctions import *
from bloomfilter import BloomFilter


class CrawledDoc:
    """A simple class that stores a crawled webpage"""
    def __init__(self, url=''):
        self.text = ''
        self.url = url


class CrawlerThread(threading.Thread):
    """A multi-thread crawler"""
    pages_count = 0     # count the number of webpages crawled
    max_pages = 10000   # the target number of webpages to crawl
    queue = Queue()     # the job queue shared among all threads
    filter = BloomFilter(1048576, False, RSHash, hash, JSHash, SDBMHash, FNVHash)
    lock = threading.Lock()
    # characters valid for use in file name, adapted from
    # valid_filename() in crawler.py
    valid_char_in_filename = '-_.() abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'

    def __init__(self, index_file):
        """
        :param index_file: a file object where each line is
        1. the URL,
        2. the name of the file that stores the source of the webpage
        corresponding to the URL, and
        3. the title of the webpage.
        """
        self.index_file = index_file
        self.doc = CrawledDoc()     # represent the webpage being crawled
        super().__init__()          # call the super constructor

    def run(self):
        cls = self.__class__        # sugar for accessing parent class
        while cls.pages_count < cls.max_pages:
            # retrieve a URL from the job queue and initialized a CrawledDoc
            self.doc = CrawledDoc(cls.queue.get())
            # Mark as done immediately because the crawler won't return to the
            cls.queue.task_done()                                   # same URL.
            if ('sjtu' in self.doc.url  # restrict to SJTU sites
                    # query the Bloom filter
                    and not self.filter.query(self.doc.url)
                    and self.is_html()):    # check the Content-Type header
                try:    # for catching request exceptions and HTTP errors
                    r = requests.get(self.doc.url, timeout=1)
                    r.raise_for_status()    # raise HTTP errors as exception

                    # The Requests module will automatically inspect the
                    # Content-Type header for encoding. If no encoding is
                    # specified, it will fall back to ISO-8859-1, per the
                    # requirement of RFC 2616, which can't be a worse idea.
                    # Pass binary data to Beautiful Soup to let it determine
                    # encoding from <meta> instead.
                    if 'charset' not in r.headers['Content-Type']:
                        soup = BeautifulSoup(r.content, 'html5lib')
                    else:
                        soup = BeautifulSoup(r.text, 'html5lib')

                    # strip the HTML of <script> and <style>, or they will be
                    # returned by get_text()
                    for tag in soup.find_all(['script', 'style']):
                        tag.decompose()
                    try:
                        # detect parsing errors after <script> and <style>
                        # were removed
                        repr(soup)
                    except RecursionError as e:
                        print(self.name + ': ' + self.doc.url + '\n   ', e)
                        continue

                    # get all the texts, stripped, separated by newline
                    self.doc.text = soup.get_text('\n', strip=True)
                    if len(self.doc.text):
                        # mark as crawled only if it has meaningful texts
                        self.filter.set(self.doc.url)
                        # output on successfully crawling a web page
                        print(self.name + ':', cls.pages_count, self.doc.url)
                        try:
                            # try to extract the title
                            title = soup.find('title').string.strip()
                        except AttributeError:
                            title = self.doc.url    # use its URL as title
                            print(self.name + ': ' + self.doc.url)
                            print('    AttributeError: the webpage doesn\'t have a title')

                        # convert a URL to a valid filename
                        filename = self.url2filename()
                        # write webpage source
                        with open('crawled/html/' + filename, 'w') as f:
                            f.write(str(soup))
                        # write texts
                        with open('crawled/text/' + filename, 'w') as f:
                            f.write(self.doc.text)

                        cls.lock.acquire()
                        # writing to the index file
                        self.index_file.write(self.doc.url
                                              + '\t' + filename
                                              + '\t' + title + '\n')

                        # add URLs in the web page to the queue
                        # Duplicate URLs are not removed in this stage.
                        for a in soup.find_all('a', href=True):
                            # remove all spaces in the href of <a>
                            link = ''.join(c for c in a['href'] if not c.isspace())
                            if link.startswith('http'):     # full URL
                                cls.queue.put(link)
                            elif link.startswith('/'):      # absolute path
                                cls.queue.put(urllib.parse.urljoin(
                                    self.doc.url, link))
                            elif len(link):                 # relative path
                                if not self.doc.url.endswith('/'):
                                    link = '/' + link
                                cls.queue.put(self.doc.url + link)
                        # increment the count after everything has been done
                        cls.pages_count += 1
                        cls.lock.release()

                # handle exceptions in request and bad responses
                except (requests.exceptions.RequestException, requests.exceptions.HTTPError) as e:
                    print(self.name + ': ' + self.doc.url + '\n   ', e)

    def is_html(self):
        """Check whether the URL stored in self.doc points to an HTML."""
        try:
            r = requests.head(self.doc.url, timeout=1)  # HEAD instead of GET
            return 'text/html' in r.headers.get('Content-Type', '')
        # handle exceptions during request
        except requests.exceptions.RequestException as e:
            print(self.name + ': ' + self.doc.url + '\n   ', e)
            return False

    def url2filename(self):
        """
        Construct a valid filename from self.doc.url, adapted from
        valid_filename() of crawler.py
        """
        return ''.join(c for c in self.doc.url if c in self.__class__.valid_char_in_filename)[:64]


if __name__ == '__main__':
    if not os.path.exists('crawled/html'):
        os.mkdir('crawled/html')
    if not os.path.exists('crawled/text'):
        os.mkdir('crawled/text')
    CrawlerThread.queue.put('https://www.sjtu.edu.cn')
    thread_pool = []
    N_THREADS = 10
    index_file = open('crawled/index.txt', 'w')

    for _ in range(N_THREADS):
        t = CrawlerThread(index_file)
        t.start()
        thread_pool.append(t)
    # It is fine not to join the thread, since the execution of the main thread
    # will not stop before each of the threads it spawned exits.
    # index_file will be closed automatically at expiration of its scope.
```

### Output

Here is an exmaple output of the crawler:

**Include an output**

```
```

The crawler will create these files:

- `crawled/html/` stores all sources of webpages
- `crawled/text/` stores all texts of webpages
- `crawled/index.txt`

`index.txt` resembles:

```
https://www.sjtu.edu.cn	httpswww.sjtu.edu.cn	上海交通大学
http://3dcampus.sjtu.edu.cn/	http3dcampus.sjtu.edu.cn	上海交通大学360度全景三维虚拟校园电子地图
https://www.sjtu.edu.cn/xbdh/yhdl/xs.htm	httpswww.sjtu.edu.cnxbdhyhdlxs.htm	在校生-上海交通大学
http://gk.sjtu.edu.cn/	httpgk.sjtu.edu.cn	首页 - 上海交通大学信息公开网
https://www.sjtu.edu.cn/xbdh/yhdl/jzg.htm	httpswww.sjtu.edu.cnxbdhyhdljzg.htm	教职工-上海交通大学
https://www.sjtu.edu.cn/xbdh/yhdl/ksjfk.htm	httpswww.sjtu.edu.cnxbdhyhdlksjfk.htm	考生及访客-上海交通大学
http://mail.sjtu.edu.cn	httpmail.sjtu.edu.cn	SJTU Single Sign On
https://www.sjtu.edu.cn/xbdh/yjdh/gk.htm	httpswww.sjtu.edu.cnxbdhyjdhgk.htm	概况-上海交通大学
https://www.sjtu.edu.cn/xbdh/yjdh/gk/xxjj.htm	httpswww.sjtu.edu.cnxbdhyjdhgkxxjj.htm	学校简介-上海交通大学
https://www.sjtu.edu.cn/xbdh/yjdh/gk/jgsz/jgbc.htm	httpswww.sjtu.edu.cnxbdhyjdhgkjgszjgbc.htm	机关部处-上海交通大学
https://www.sjtu.edu.cn/xbdh/yjdh/gk/lrld.htm	httpswww.sjtu.edu.cnxbdhyjdhgklrld.htm	历任领导-上海交通大学
https://www.sjtu.edu.cn/xbdh/yjdh/gk/lsyg.htm	httpswww.sjtu.edu.cnxbdhyjdhgklsyg.htm	历史沿革-上海交通大学
```
