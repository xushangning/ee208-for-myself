# Lab Report 5

517030910384 徐尚宁

## Introduction

In this lab report, we present a simple search engine, built on top of PyLucene. The search engine is capable of searching both webpages and images by title or description. Search results can be restricted to a particular domain by specifying `site` in the query.

The report is organized as follows: we first introduce the environment setup of PyLucene, and then present a rewritten version of the web crawler. After gathering webpages with the crawler, we build an index and search over the index for results. The crawler, the indexing and searching procedures are slightly modified to cater to the need of an image search engine, whose data are sourced from [Duokan](https://www.duokan.com).

## Environment Setup

The following list is the base environment on which PyLucene is installed:

- Arch Linux
- Python 3.7.1
- Java OpenJDK 10.0.2

The first step is to install JCC, a compile-time dependency of PyLucene. However, a simple `pip install jcc` won't work because the PyPI doesn't have binary files for JCC and auto-building fails. On [JCC PyPi](https://pypi.org/project/JCC/):

> JCC’s setup.py file needs to be edited before building JCC to specify the location of the Java Runtime Environment’s header files and libraries.

A close inspection reveals that `setup.py` should be modified in several places:

The following code configure the JDK directory for various platforms.

```python
JDK = {
    'darwin': JAVAHOME or JAVAFRAMEWORKS,
    'ipod': '/usr/include/gcc',
    'linux': '/usr/lib/jvm/java-8-oracle',  # <- Linux here
    'sunos5': '/usr/jdk/instances/jdk1.6.0',
    'win32': JAVAHOME,
    'mingw32': JAVAHOME,
    'freebsd7': '/usr/local/diablo-jdk1.6.0'
}
```

It assumes Oracle JDK 8 for Linux, which is false in our case and is changed to

```python
JDK = {
    ...
    'linux': '/usr/lib/jvm/java-10-openjdk'
    ...
}
```

Also note the dict `LFLAGS`, which are used by GCC to link libraries:

```python
LFLAGS = {
    'darwin/frameworks': ['-framework', 'JavaVM', '-mmacosx-version-min=10.9']
    'darwin/home': ['-L%(darwin)s/jre/lib' %(JDK), '-ljava',
                    '-L%(darwin)s/jre/lib/server' %(JDK), '-ljvm',
                    '-Wl,-rpath', '-Wl,%(darwin)s/jre/lib' %(JDK),
                    '-Wl,-rpath', '-Wl,%(darwin)s/jre/lib/server' %(JDK),
                    '-mmacosx-version-min=10.9'],
    'ipod': ['-ljvm', '-lpython%s.%s' %(sys.version_info[0:2]),
             '-L/usr/lib/gcc/arm-apple-darwin9/4.0.1'],
    'linux/i386': ['-L%(linux)s/jre/lib/i386' %(JDK), '-ljava',
                    '-L%(linux)s/jre/lib/i386/client' %(JDK), '-ljvm',
                    '-Wl,-rpath=%(linux)s/jre/lib/i386:%(linux)s/jre/lib/i386/client' %(JDK)],
    'linux/i686': ['-L%(linux)s/jre/lib/i386' %(JDK), '-ljava',
                    '-L%(linux)s/jre/lib/i386/client' %(JDK), '-ljvm',
                    '-Wl,-rpath=%(linux)s/jre/lib/i386:%(linux)s/jre/lib/i386/client' %(JDK)],
    'linux/x86_64': ['-L%(linux)s/jre/lib/amd64' %(JDK), '-ljava',
                      '-L%(linux)s/jre/lib/amd64/server' %(JDK), '-ljvm',
                      '-Wl,-rpath=%(linux)s/jre/lib/amd64:%(linux)s/jre/lib/amd64/server' %(JDK)],
    'sunos5': ['-L%(sunos5)s/jre/lib/i386' %(JDK), '-ljava',
               '-L%(sunos5)s/jre/lib/i386/client' %(JDK), '-ljvm',
               '-R%(sunos5)s/jre/lib/i386:%(sunos5)s/jre/lib/i386/client' %(JDK)],
    'win32': ['/DLL', '/LIBPATH:%(win32)s/lib' %(JDK), 'Ws2_32.lib', 'jvm.lib'],
    'mingw32': ['-L%(mingw32)s/lib' %(JDK), '-ljvm'],
    'freebsd7': ['-L%(freebsd7)s/jre/lib/i386' %(JDK), '-ljava', '-lverify',
                 '-L%(freebsd7)s/jre/lib/i386/client' %(JDK), '-ljvm',
                 '-Wl,-rpath=%(freebsd7)s/jre/lib/i386:%(freebsd7)s/jre/lib/i386/client' %(JDK)],
}
```

In the key `linux/x86_64`, `setup.py` assumes that library files are located under `JDK['linux'] + '/jre/lib/amd64` i.e. `/usr/lib/jvm/java-10-openjdk/jre/lib/amd64/'`, where `JDK` refers to the `dict` just mentioned. For OpenJDK 10, the library files are moved to `JDK['linux'] + '/lib/'` i.e. just `/usr/lib/jvm/java-10-openjdk/lib/`.

After modifications to `LFLAGS`, the dict is

```python
LFLAGS = {
    ...
    'linux/x86_64': ['-L%(linux)s/lib' %(JDK), '-ljava',
                      '-L%(linux)s/lib/server' %(JDK), '-ljvm',
                      '-Wl,-rpath=%(linux)s/jre/lib:%(linux)s/jre/lib/server' %(JDK)],
    ...
}
```

There is one more place to configure. PyLucene requires that JCC supports compiling in "shared mode". During compilation of JCC, shared mode support is only enabled if `setup.py` detects a "modern setuptools" on the OS:

```python
try:
    from pkg_resources import SetuptoolsVersion
    with_modern_setuptools = True
except ImportError:
    try:
        from pkg_resources.extern.packaging.version import Version
        with_modern_setuptools = True
    except ImportError:
        with_modern_setuptools = False
```

Due to customizations of Setuptools at Arch Linux part, the line

```python
from pkg_resources.extern.packaging.version import Version
```

fails because `packaging.version` is available as a separate module instead of a sub-module of `pkg_resources`, thus throwing an `ImportError`. The fix is simple:

```python
from packaging.version import Version
```

Now we are done with `setup.py`, it is time to run `setup.py build` and `setup.py install`.

Next step is to install PyLucene. Uncomment the following lines in `Makefile`:

```makefile
# Linux     (Debian Jessie 64-bit, Python 3.4.2, Oracle Java 1.8                
# Be sure to also set JDK['linux'] in jcc's setup.py to the JAVA_HOME value     
# used below for ANT (and rebuild jcc after changing it).                       
#PREFIX_PYTHON=/usr
#ANT=JAVA_HOME=/usr/lib/jvm/java-8-oracle /usr/bin/ant
#PYTHON=$(PREFIX_PYTHON)/bin/python3
#JCC=$(PYTHON) -m jcc --shared
#NUM_FILES=8
```

and make changes to `ANT=JAVA_HOME=` so that it points to `/usr/lib/jvm/java-10-openjdk`. Before compiling, make sure that Ant is installed.

The final step is running `make` and `make install`. Verify the installation with:

```python
>>> import lucene
>>> lucene.initVM()
```

## Rewriting the Crawler

### The Problem

We identify several important issues with the original crawler:

1. Difficulty of debugging. It is impossible to identify each thread with a unique name, so it is difficult to tell which thread has ended due to an (uncaught) exception or simply, the webpage is crawled by which thread.
2. Non-existent encoding handling.
3. Detection of the content type of response is missing. Threads are often occupied by a lengthy response e.g. a PDF file download.
4. Unclear execution logic. The multi-thread crawler in `parallel_crawler.py` is built on `crawler.py`, which is based on the `crawle.py` included in the lab materials. The single-threaded `crawler.py` is no longer used and serves only as a base for `parallel_crawler.py`. In `parallel_crawler.py` you can see:
    - dangling functions imported from `crawler.py`
    - a function definition `crawling` inserted between statements. This is deliberate to allow `crawling()` to access global variables shared by all threads.

These problems are highlighted in `parallel_crawler.py` below.

```python
import threading
from queue import Queue

from bloomfilter import BloomFilter
import crawler
from GeneralHashFunctions import *

seed = 'https://www.sjtu.edu.cn'
q = Queue()
max_page = 2000
count_pages = 0
graph = {}

BIT_ARRAY_SIZE = 16384
my_filter = BloomFilter(BIT_ARRAY_SIZE, hash, RSHash, JSHash, SDBMHash, FNVHash)

N_THREADS = 4
var_lock = threading.Lock()     # <- main execution


def crawling():                 # <- function definition
    global count_pages  # <- access global variables
    while count_pages < max_page:   # <- access global variables
        print(count_pages, end=' ')
        url = q.get()
        if not my_filter.query(url):
            print(url)
            content = crawler.get_page(url) # <- imported from crawler
            if len(content):
                my_filter.set(url)
                crawler.add_page_to_folder(url, content)
                outlinks = crawler.get_all_links(content, url)
                var_lock.acquire()  # ready to change the global environment
                count_pages += 1
                if len(outlinks):
                    graph[url] = outlinks
                    for link in outlinks:
                        q.put(link)
                var_lock.release()
        q.task_done()


q.put(seed)                     # <- main execution again
thread_pool = []
for _ in range(N_THREADS):
    t = threading.Thread(target=crawling)
    t.start()
    thread_pool.append(t)
for t in thread_pool:
    t.join()

my_filter.print_stats()
print("Number of Crawled Pages:", count_pages)
```

### New `parallel_crawler.py`

There are some distinct features for the new `parallel_crawler.py`:

- The working function `crawling()` becomes a thing of the past. We define a `CrawlerThread` class that inherits `threading.Thread` and overwrite `threading.Thread.run()`.
- Variables shared among threads are now class variables of `CrawlerThread`. Examples of shared variables are the Bloom filter, the job queue and `count_pages`.
- Each thread is identified by their name of the thread.
- Detection of encoding and the content type of a web page.

The new `parallel_crawler.py` is reproduced below with extensive comments.

```python
import os
import threading
from queue import Queue
import urllib.parse

import requests
from bs4 import BeautifulSoup

from GeneralHashFunctions import *
from bloomfilter import BloomFilter


class CrawledDoc:
    """A simple class that stores a crawled webpage"""
    def __init__(self, url=''):
        self.text = ''
        self.url = url


class CrawlerThread(threading.Thread):
    """A multi-thread crawler"""
    pages_count = 0     # count the number of webpages crawled
    max_pages = 10000   # the target number of webpages to crawl
    queue = Queue()     # the job queue shared among all threads
    filter = BloomFilter(1048576, False, RSHash, hash, JSHash, SDBMHash, FNVHash)
    lock = threading.Lock()
    # characters valid for use in file name, adapted from
    # valid_filename() in crawler.py
    valid_char_in_filename = '-_.() abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'

    def __init__(self, index_file):
        """
        :param index_file: a file object where each line is
        1. the URL,
        2. the name of the file that stores the source of the webpage
        corresponding to the URL, and
        3. the title of the webpage.
        """
        self.index_file = index_file
        self.doc = CrawledDoc()     # represent the webpage being crawled
        super().__init__()          # call the super constructor

    def run(self):
        cls = self.__class__        # sugar for accessing parent class
        while cls.pages_count < cls.max_pages:
            # retrieve a URL from the job queue and initialized a CrawledDoc
            self.doc = CrawledDoc(cls.queue.get())
            # Mark as done immediately because the crawler won't return to the
            cls.queue.task_done()                                   # same URL.
            if ('sjtu' in self.doc.url  # restrict to SJTU sites
                    # query the Bloom filter
                    and not self.filter.query(self.doc.url)
                    and self.is_html()):    # check the Content-Type header
                try:    # for catching request exceptions and HTTP errors
                    r = requests.get(self.doc.url, timeout=1)
                    r.raise_for_status()    # raise HTTP errors as exception

                    # The Requests module will automatically inspect the
                    # Content-Type header for encoding. If no encoding is
                    # specified, it will fall back to ISO-8859-1, per the
                    # requirement of RFC 2616, which can't be a worse idea.
                    # Pass binary data to Beautiful Soup to let it determine
                    # encoding from <meta> instead.
                    if 'charset' not in r.headers['Content-Type']:
                        soup = BeautifulSoup(r.content, 'html5lib')
                    else:
                        soup = BeautifulSoup(r.text, 'html5lib')

                    # strip the HTML of <script> and <style>, or they will be
                    # returned by get_text()
                    for tag in soup.find_all(['script', 'style']):
                        tag.decompose()
                    try:
                        # detect parsing errors after <script> and <style>
                        # were removed
                        repr(soup)
                    except RecursionError as e:
                        print(self.name + ': ' + self.doc.url + '\n   ', e)
                        continue

                    # get all the texts, stripped, separated by newline
                    self.doc.text = soup.get_text('\n', strip=True)
                    if len(self.doc.text):
                        # mark as crawled only if it has meaningful texts
                        self.filter.set(self.doc.url)
                        # output on successfully crawling a web page
                        print(self.name + ':', cls.pages_count, self.doc.url)
                        try:
                            # try to extract the title
                            title = soup.find('title').string.strip()
                        except AttributeError:
                            title = self.doc.url    # use its URL as title
                            print(self.name + ': ' + self.doc.url)
                            print('    AttributeError: the webpage doesn\'t have a title')

                        # convert a URL to a valid filename
                        filename = self.url2filename()
                        # write webpage source
                        with open('crawled/html/' + filename, 'w') as f:
                            f.write(str(soup))
                        # write texts
                        with open('crawled/text/' + filename, 'w') as f:
                            f.write(self.doc.text)

                        cls.lock.acquire()
                        # writing to the index file
                        self.index_file.write(self.doc.url
                                              + '\t' + filename
                                              + '\t' + title + '\n')

                        # add URLs in the web page to the queue
                        # Duplicate URLs are not removed in this stage.
                        for a in soup.find_all('a', href=True):
                            # remove all spaces in the href of <a>
                            link = ''.join(c for c in a['href'] if not c.isspace())
                            if link.startswith('http'):     # full URL
                                cls.queue.put(link)
                            elif link.startswith('/'):      # absolute path
                                cls.queue.put(urllib.parse.urljoin(
                                    self.doc.url, link))
                            elif len(link):                 # relative path
                                if not self.doc.url.endswith('/'):
                                    link = '/' + link
                                cls.queue.put(self.doc.url + link)
                        # increment the count after everything has been done
                        cls.pages_count += 1
                        cls.lock.release()

                # handle exceptions in request and bad responses
                except (requests.exceptions.RequestException, requests.exceptions.HTTPError) as e:
                    print(self.name + ': ' + self.doc.url + '\n   ', e)

    def is_html(self):
        """Check whether the URL stored in self.doc points to an HTML."""
        try:
            r = requests.head(self.doc.url, timeout=1)  # HEAD instead of GET
            return 'text/html' in r.headers.get('Content-Type', '')
        # handle exceptions during request
        except requests.exceptions.RequestException as e:
            print(self.name + ': ' + self.doc.url + '\n   ', e)
            return False

    def url2filename(self):
        """
        Construct a valid filename from self.doc.url, adapted from
        valid_filename() of crawler.py
        """
        return ''.join(c for c in self.doc.url if c in self.__class__.valid_char_in_filename)[:64]


if __name__ == '__main__':
    if not os.path.exists('crawled/html'):
        os.mkdir('crawled/html')
    if not os.path.exists('crawled/text'):
        os.mkdir('crawled/text')
    CrawlerThread.queue.put('https://www.sjtu.edu.cn')
    thread_pool = []
    N_THREADS = 10
    index_file = open('crawled/index.txt', 'w')

    for _ in range(N_THREADS):
        t = CrawlerThread(index_file)
        t.start()
        thread_pool.append(t)
    # It is fine not to join the thread, since the execution of the main thread
    # will not stop before each of the threads it spawned exits.
    # index_file will be closed automatically at expiration of its scope.
```

### Output

Here is an exmaple output of the crawler:

```
Thread-1: 0 https://www.sjtu.edu.cn
Thread-9: http://www.jwc.sjtu.edu.cn/web/sjtu/198109.htm
    HTTPConnectionPool(host='www.jwc.sjtu.edu.cn', port=80): Max retries exceeded with url: /web/sjtu/198109.htm (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa90c78e320>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
Thread-2: 1 http://3dcampus.sjtu.edu.cn/
Thread-5: http://en.sjtu.edu.cn/
    403 Client Error: Forbidden for url: http://en.sjtu.edu.cn/
Thread-8: 2 http://gk.sjtu.edu.cn/
```

The crawler will create these files:

- `crawled/html/` stores all sources of webpages
- `crawled/text/` stores all texts of webpages
- `crawled/index.txt`

`index.txt` resembles:

```
https://www.sjtu.edu.cn	httpswww.sjtu.edu.cn	上海交通大学
http://3dcampus.sjtu.edu.cn/	http3dcampus.sjtu.edu.cn	上海交通大学360度全景三维虚拟校园电子地图
https://www.sjtu.edu.cn/xbdh/yhdl/xs.htm	httpswww.sjtu.edu.cnxbdhyhdlxs.htm	在校生-上海交通大学
http://gk.sjtu.edu.cn/	httpgk.sjtu.edu.cn	首页 - 上海交通大学信息公开网
https://www.sjtu.edu.cn/xbdh/yhdl/jzg.htm	httpswww.sjtu.edu.cnxbdhyhdljzg.htm	教职工-上海交通大学
https://www.sjtu.edu.cn/xbdh/yhdl/ksjfk.htm	httpswww.sjtu.edu.cnxbdhyhdlksjfk.htm	考生及访客-上海交通大学
http://mail.sjtu.edu.cn	httpmail.sjtu.edu.cn	SJTU Single Sign On
https://www.sjtu.edu.cn/xbdh/yjdh/gk.htm	httpswww.sjtu.edu.cnxbdhyjdhgk.htm	概况-上海交通大学
https://www.sjtu.edu.cn/xbdh/yjdh/gk/xxjj.htm	httpswww.sjtu.edu.cnxbdhyjdhgkxxjj.htm	学校简介-上海交通大学
https://www.sjtu.edu.cn/xbdh/yjdh/gk/lrld.htm	httpswww.sjtu.edu.cnxbdhyjdhgklrld.htm	历任领导-上海交通大学
https://www.sjtu.edu.cn/xbdh/yjdh/gk/lsyg.htm	httpswww.sjtu.edu.cnxbdhyjdhgklsyg.htm	历史沿革-上海交通大学
```

With the rewritten crawler, running 10 threads, we gather around 10 000 webpages in less than half an hour to build our search engine.

## Indexing and Search Webpages

### Converting Provided Examples

Examples provided are written in Python 2 with Lucene 4.9.0 and should be converted to Python 3. Converting from Python 2 to 3 is done with the `2to3` command line utility:

```bash
$ 2to3 -w IndexFiles.py
```

Converting from Lucene 4.9.0 to 7.5.0 is mostly about deleting `Version.LUCENE_CURRENT` from function parameters. To give an idea of the process, here is a Git diff between the original and converted `IndexFiles.py`:

```diff
diff --git a/IndexFiles.py b/IndexFiles.py
index da7700c..57a81f3 100644
--- a/IndexFiles.py
+++ b/IndexFiles.py
@@ -1,17 +1,16 @@
-#!/usr/bin/env python
-
-INDEX_DIR = "IndexFiles.index"
-
-import sys, os, lucene, threading, time
+import lucene
+import os
+import sys
+import threading
+import time
 from datetime import datetime
 
 from java.io import File
 from org.apache.lucene.analysis.miscellaneous import LimitTokenCountAnalyzer
 from org.apache.lucene.analysis.standard import StandardAnalyzer
 from org.apache.lucene.document import Document, Field, FieldType
-from org.apache.lucene.index import FieldInfo, IndexWriter, IndexWriterConfig
+from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexOptions
 from org.apache.lucene.store import SimpleFSDirectory
-from org.apache.lucene.util import Version
 
 """
 This class is loosely based on the Lucene (java implementation) demo class 
@@ -22,6 +21,9 @@ resulting Lucene index will be placed in the current directory and called
 'index'.
 """
 
+INDEX_DIR = "IndexFiles.index"
+
+
 class Ticker(object):
 
     def __init__(self):
@@ -33,6 +35,7 @@ class Ticker(object):
             sys.stdout.flush()
             time.sleep(1.0)
 
+
 class IndexFiles(object):
     """Usage: python IndexFiles <doc_directory>"""
 
@@ -41,10 +44,10 @@ class IndexFiles(object):
         if not os.path.exists(storeDir):
             os.mkdir(storeDir)
 
-        store = SimpleFSDirectory(File(storeDir))
-        analyzer = StandardAnalyzer(Version.LUCENE_CURRENT)
+        store = SimpleFSDirectory(File(storeDir).toPath())
+        analyzer = StandardAnalyzer()
         analyzer = LimitTokenCountAnalyzer(analyzer, 1048576)
-        config = IndexWriterConfig(Version.LUCENE_CURRENT, analyzer)
+        config = IndexWriterConfig(analyzer)
         config.setOpenMode(IndexWriterConfig.OpenMode.CREATE)
         writer = IndexWriter(store, config)
 
@@ -60,15 +63,13 @@ class IndexFiles(object):
     def indexDocs(self, root, writer):
 
         t1 = FieldType()
-        t1.setIndexed(False)
         t1.setStored(True)
         t1.setTokenized(False)
         
         t2 = FieldType()
-        t2.setIndexed(True)
         t2.setStored(False)
         t2.setTokenized(True)
-        t2.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)
+        t2.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)
         
         for root, dirnames, filenames in os.walk(root):
             for filename in filenames:
@@ -78,7 +79,7 @@ class IndexFiles(object):
                 try:
                     path = os.path.join(root, filename)
                     file = open(path)
-                    contents = str(file.read(), 'gbk')
+                    contents = file.read()
                     file.close()
                     doc = Document()
                     doc.add(Field("name", filename, t1))
@@ -91,12 +92,14 @@ class IndexFiles(object):
                 except Exception as e:
                     print("Failed in indexDocs:", e)
 
+
 if __name__ == '__main__':
     lucene.initVM(vmargs=['-Djava.awt.headless=true'])
     print('lucene', lucene.VERSION)
     start = datetime.now()
     try:
-        IndexFiles('testfolder', "index")
+        prefix = 'instructions/4/samples4/'
+        IndexFiles(prefix + 'testfolder', "index")
         end = datetime.now()
         print(end - start)
     except Exception as e:
```

### Indexing

`IndexFiles.py` mostly follows the pattern set out in the example. It opens a directory for writing index, opens `crawled/index.txt` to get a list of all crawled webpages and follow the order laid out in `index.txt` to add each webpage to the index:

```python
import lucene
import os
import sys
import threading
import time
from datetime import datetime
import jieba
from urllib.parse import urlparse

from java.io import File
from org.apache.lucene.analysis.miscellaneous import LimitTokenCountAnalyzer
from org.apache.lucene.analysis.core import WhitespaceAnalyzer
from org.apache.lucene.document import Document, Field, FieldType
from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexOptions
from org.apache.lucene.store import SimpleFSDirectory


class Ticker(object):

    def __init__(self):
        self.tick = True

    def run(self):
        while self.tick:
            sys.stdout.write('.')
            sys.stdout.flush()
            time.sleep(1.0)


class IndexFiles(object):
    """Usage: python IndexFiles <doc_directory>"""

    def __init__(self, storeDir):

        if not os.path.exists(storeDir):
            os.mkdir(storeDir)

        store = SimpleFSDirectory(File(storeDir).toPath())
        analyzer = WhitespaceAnalyzer()
        analyzer = LimitTokenCountAnalyzer(analyzer, 1048576)
        config = IndexWriterConfig(analyzer)
        config.setOpenMode(IndexWriterConfig.OpenMode.CREATE)
        writer = IndexWriter(store, config)

        self.indexDocs(writer)
        ticker = Ticker()
        print('commit index', end=' ')
        threading.Thread(target=ticker.run).start()
        writer.commit()
        writer.close()
        ticker.tick = False
        print('done')

    def indexDocs(self, writer):
        # the field type for filename and URL
        filename_fieldtype = FieldType()
        filename_fieldtype.setStored(True)
        filename_fieldtype.setTokenized(False)
        
        content_fieldtype = FieldType()
        content_fieldtype.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)

        # Titles should not only be indexed and tokenized, but also stored to be
        # returned as search results
        title_fieldtype = FieldType(content_fieldtype)
        title_fieldtype.setStored(True)

        # the field type for sites
        site_fieldtype = FieldType()
        site_fieldtype.setStored(True)
        site_fieldtype.setTokenized(False)
        site_fieldtype.setIndexOptions(IndexOptions.DOCS_AND_FREQS)

        index_file = open('crawled/index.txt')
        for line in index_file:
            try:
                url, filename, title = line.rstrip('\n').split('\t')
            # Unpacking errors come from spaces in URLs.
            except ValueError:          # skip unpacking error
                continue
            print('Adding ' + filename)
            doc = Document()

            with open('crawled/text/' + filename, 'r') as f:
                try:
                    # use jieba to cut texts into words
                    content = ' '.join(jieba.cut_for_search(f.read()))
                except UnicodeDecodeError:
                    continue            # skip decoding errors
                doc.add(Field('content', content, content_fieldtype))
            doc.add(Field('filename', filename, filename_fieldtype))
            doc.add(Field('title', ' '.join(jieba.cut_for_search(title)), title_fieldtype))
            # parse URLs for their domains
            doc.add(Field('site', urlparse(url).netloc, site_fieldtype))
            doc.add(Field('url', url, filename_fieldtype))
            writer.addDocument(doc)


if __name__ == '__main__':
    lucene.initVM(vmargs=['-Djava.awt.headless=true'])
    print('lucene', lucene.VERSION)
    start = datetime.now()
    try:
        # index files and store the indices in `index/`
        IndexFiles('index')
        end = datetime.now()
        print(end - start)
    except Exception as e:
        print("Failed: ", e)
        raise e
```

Adding all 10 000 webpages to index takes around 45 seconds.

### Searching

Searching centers around the `QueryParser` class. Although `QueryParser` is able to pasre queries like `title:cats`, we still decide to use the homebrew `parse_query()` function based on `parseCommand()` from the example. The rationale behind this is that `QueryParser` parses queries with the analyzer supplied as a function parameter. Since no tokenizer in the built-in analyzers is able to correctly cut Chinese into words, the query will be used as-is for searching. Without `QueryParser`, we have to map terms to fields ourselves with `parser_query()`. The `SearchFiles.py` is as follows:

```python
import jieba
import lucene
from java.io import File
from org.apache.lucene.analysis.core import WhitespaceAnalyzer
from org.apache.lucene.index import DirectoryReader, Term
from org.apache.lucene.queryparser.classic import QueryParser
from org.apache.lucene.search import IndexSearcher, BooleanQuery, BooleanClause, TermQuery
from org.apache.lucene.store import SimpleFSDirectory


def parse_query(query):
    """
    Parse queries in the form "<term>* (<field>:<term>)*" to a dict
    :param query: str. Terms whose field is not specified will be gathered in
    the "default" field
    :return: dict
    """
    allowed_fields = {'title', 'content', 'site'}
    query_dict = {}
    for i in query.split(' '):
        if ':' in i:
            opt, value = i.split(':')[:2]
            opt = opt.lower()
            # Currently, if there are multiple occurrences of the same field,
            # only the last term will be added to the query.
            if opt in allowed_fields and len(value):
                query_dict[opt] = value
        else:
            # gather terms without fields under the "default" key
            query_dict['default'] = query_dict.get('default', '') + ' ' + i
    return query_dict


if __name__ == '__main__':
    STORE_DIR = "index"
    lucene.initVM(vmargs=['-Djava.awt.headless=true'])
    print('lucene', lucene.VERSION)
    #base_dir = os.path.dirname(os.path.abspath(sys.argv[0]))
    directory = SimpleFSDirectory(File(STORE_DIR).toPath())
    searcher = IndexSearcher(DirectoryReader.open(directory))
    analyzer = WhitespaceAnalyzer()

    # gathering all field names together for output formatting later
    field_names = ('url', 'title', 'site', 'filename')
    while True:
        print()
        print("Hit enter with no input to quit.")
        command = input("Query:")
        if command == '':
            break

        print('\nSearching for:', command)
        # In later version of Lucene, BooleanQuery becomes immutable and can
        # only be constructed by BooleanQuery.Builder
        query_builder = BooleanQuery.Builder()
        query_dict = parse_query(command)
        if query_dict.get('default') is not None:
            terms = ' '.join(jieba.cut(query_dict['default']))
            # search over "content" and "title' fields
            query = QueryParser('content', analyzer).parse(terms)
            query_builder.add(query, BooleanClause.Occur.SHOULD)
            query = QueryParser('title', analyzer).parse(terms)
            query_builder.add(query, BooleanClause.Occur.SHOULD)
        if query_dict.get('site') is not None:
            # use TermQuery because we expect an exact match
            query_builder.add(TermQuery(Term('site', query_dict['site'])),
                              BooleanClause.Occur.MUST)
        # build a BooleanQuery object and pass it to searcher
        # retrieve the top 20 hits
        scoreDocs = searcher.search(query_builder.build(), 20).scoreDocs

        print(len(scoreDocs), 'total matching documents.')
        for i, scoreDoc in enumerate(scoreDocs):
            doc = searcher.doc(scoreDoc.doc)
            for n in field_names:
                # get the value stored in the field
                print(n + ':', doc.get(n))
            print('score:', scoreDoc.score, end='\n\n')
            # print 'explain:', searcher.explain(query, scoreDoc.doc)
```

### Results

It is notable that compared with searching over web content only, search over both page content and title gives a significant boost to some queries:

Here we are searching for 上海交通大学. The top five for searching only web contents are:

```
Searching for: 上海交通大学
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.617 seconds.
Prefix dict has been built succesfully.
20 total matching documents.
url: http://xxgk.sjtu.edu.cn/infogklm/zdgf/zhangchengzhidu/
title: 学校 各类 规章 制度 规章制度 - 上海 交通 大学 上海交通大学 信息 公开 网
site: xxgk.sjtu.edu.cn
filename: httpxxgk.sjtu.edu.cninfogklmzdgfzhangchengzhidu
score: 0.854279637336731

url: http://media.sjtu.edu.cn
title: 新 媒体
site: media.sjtu.edu.cn
filename: httpmedia.sjtu.edu.cn
score: 0.8418781757354736

url: http://xxgk.sjtu.edu.cn/infogklm/wzsbcggl/
title: 物资 设备 采购 管理 - 上海 交通 大学 上海交通大学 信息 公开 网
site: xxgk.sjtu.edu.cn
filename: httpxxgk.sjtu.edu.cninfogklmwzsbcggl
score: 0.8418470621109009

url: http://px.sjtu.edu.cn/
title: 上海 交通 大学 上海交通大学 培训 中心 培训中心 / 上海 交通 大学 上海交通大学 干部 培训 / 上海 交通 大学 上海交通大学 教育 培训 - 上海 交通 大学 上海交通大学 继续 教育 学院 高端 管理 培训 中心 培训中心 官网
site: px.sjtu.edu.cn
filename: httppx.sjtu.edu.cn
score: 0.841398298740387

url: http://postd.sjtu.edu.cn/
title: 欢迎 光临 欢迎光临 上海 交通 大学 上海交通大学 博士 博士后 管理 办公 公室 办公室 ！
site: postd.sjtu.edu.cn
filename: httppostd.sjtu.edu.cn
score: 0.840452253818512
```

While here are the top five for search both content and title:

```
Query:上海交通大学

Searching for: 上海交通大学
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.605 seconds.
Prefix dict has been built succesfully.
20 total matching documents.
url: https://www.sjtu.edu.cn/index.htm
title: 上海 交通 大学 上海交通大学
site: www.sjtu.edu.cn
filename: httpswww.sjtu.edu.cnindex.htm
score: 1.7977747917175293

url: https://www.sjtu.edu.cn
title: 上海 交通 大学 上海交通大学
site: www.sjtu.edu.cn
filename: httpswww.sjtu.edu.cn
score: 1.7869325876235962

url: https://www.sjtu.edu.cn/
title: 上海 交通 大学 上海交通大学
site: www.sjtu.edu.cn
filename: httpswww.sjtu.edu.cn
score: 1.7869325876235962

url: http://px.sjtu.edu.cn/
title: 上海 交通 大学 上海交通大学 培训 中心 培训中心 / 上海 交通 大学 上海交通大学 干部 培训 / 上海 交通 大学 上海交通大学 教育 培训 - 上海 交通 大学 上海交通大学 继续 教育 学院 高端 管理 培训 中心 培训中心 官网
site: px.sjtu.edu.cn
filename: httppx.sjtu.edu.cn
score: 1.7764580249786377

url: http://yzb.sjtu.edu.cn/
title: 上海 交通 大学 上海交通大学 研招网
site: yzb.sjtu.edu.cn
filename: httpyzb.sjtu.edu.cn
score: 1.7752482891082764
```

In fact, the official website for SJTU doesn't appear in the first search results. It is very likely that searching 上海交通大学 in title yields an exact match for the official website. How about searching for 网管部, a lesser-known department in SJTU:

```
Searching for: 网管部
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.614 seconds.
Prefix dict has been built succesfully.
20 total matching documents.
url: http://cuti.sjtu.edu.cn/n/354
title: 2017 年度 优秀 员工 优秀员工 — — 顾斌 - 交 大海 科
site: cuti.sjtu.edu.cn
filename: httpcuti.sjtu.edu.cnn354
score: 8.71887493133545

url: http://cuti.sjtu.edu.cn/n/354
title: 2017 年度 优秀 员工 优秀员工 — — 顾斌 - 交 大海 科
site: cuti.sjtu.edu.cn
filename: httpcuti.sjtu.edu.cnn354
score: 8.71887493133545

url: http://digitalarchives.sjtu.edu.cn/info/1120/5029.htm
title: 老 交大 技击 运动 的 兴起 与 发展 - 档案 档案馆 （ 党史 校史 研究 研究室 ） 主页
site: digitalarchives.sjtu.edu.cn
filename: httpdigitalarchives.sjtu.edu.cninfo11205029.htm
score: 6.285496234893799

url: http://houqin.sjtu.edu.cn/cview.asp?blto=6&did=184&mytype=361&id=320
title: 上海 交通 大学 上海交通大学 后勤 保障 后勤保障 中心 - > 下属 机构
site: houqin.sjtu.edu.cn
filename: httphouqin.sjtu.edu.cncview.aspblto6did184mytype361id320
score: 6.214733123779297

url: http://www.lib.sjtu.edu.cn/index.php?m=content&c=index&a=lists&catid=51
title: 上海 交通 大学 上海交通大学 图书 书馆 图书馆
site: www.lib.sjtu.edu.cn
filename: httpwww.lib.sjtu.edu.cnindex.phpmcontentcindexalistscatid51
score: 6.209000110626221
```

```
Searching for: 网管部
20 total matching documents.
url: http://nimo.sjtu.edu.cn/
title: SJTU - NIMO !   上海 交通 大学 上海交通大学 学生 网络 信息 管理 信息管理 部
site: nimo.sjtu.edu.cn
filename: httpnimo.sjtu.edu.cn
score: 13.067777633666992

url: http://cuti.sjtu.edu.cn/n/354
title: 2017 年度 优秀 员工 优秀员工 — — 顾斌 - 交 大海 科
site: cuti.sjtu.edu.cn
filename: httpcuti.sjtu.edu.cnn354
score: 8.71887493133545

url: http://cuti.sjtu.edu.cn/n/354
title: 2017 年度 优秀 员工 优秀员工 — — 顾斌 - 交 大海 科
site: cuti.sjtu.edu.cn
filename: httpcuti.sjtu.edu.cnn354
score: 8.71887493133545

url: http://digitalarchives.sjtu.edu.cn/info/1120/5029.htm
title: 老 交大 技击 运动 的 兴起 与 发展 - 档案 档案馆 （ 党史 校史 研究 研究室 ） 主页
site: digitalarchives.sjtu.edu.cn
filename: httpdigitalarchives.sjtu.edu.cninfo11205029.htm
score: 6.285496234893799

url: http://houqin.sjtu.edu.cn/cview.asp?blto=6&did=184&mytype=361&id=320
title: 上海 交通 大学 上海交通大学 后勤 保障 后勤保障 中心 - > 下属 机构
site: houqin.sjtu.edu.cn
filename: httphouqin.sjtu.edu.cncview.aspblto6did184mytype361id320
score: 6.214733123779297
```

Without an exact match, the official website still manages to climb to the top. The preliminary conclusion may be that the page title should recieve more weight in ranking.

The last part in this section is showing the site search:

```
Searching for: site:abc.sjtu.edu.cn
20 total matching documents.
url: http://abc.sjtu.edu.cn/
title: 上海 交通 大学 上海交通大学 - 现代 农业 现代农业 与 生物 工程 生物工程 训练 中心
site: abc.sjtu.edu.cn
filename: httpabc.sjtu.edu.cn
score: 5.509438514709473

url: http://abc.sjtu.edu.cn/centerOverview/agencies.aspx
title: 上海 交通 大学 上海交通大学 - 现代 农业 现代农业 与 生物 工程 生物工程 训练 中心
site: abc.sjtu.edu.cn
filename: httpabc.sjtu.edu.cncenterOverviewagencies.aspx
score: 5.509438514709473

url: http://abc.sjtu.edu.cn/centerOverview/directorIntro.aspx
title: 上海 交通 大学 上海交通大学 - 现代 农业 现代农业 与 生物 工程 生物工程 训练 中心
site: abc.sjtu.edu.cn
filename: httpabc.sjtu.edu.cncenterOverviewdirectorIntro.aspx
score: 5.509438514709473

url: http://abc.sjtu.edu.cn/centerOverview/centerIntro.aspx
title: 上海 交通 大学 上海交通大学 - 现代 农业 现代农业 与 生物 工程 生物工程 训练 中心
site: abc.sjtu.edu.cn
filename: httpabc.sjtu.edu.cncenterOverviewcenterIntro.aspx
score: 5.509438514709473

url: http://abc.sjtu.edu.cn/syjx/sykcIntro1.aspx
title: 上海 交通 大学 上海交通大学 - 现代 农业 现代农业 与 生物 工程 生物工程 训练 中心
site: abc.sjtu.edu.cn
filename: httpabc.sjtu.edu.cnsyjxsykcIntro1.aspx
score: 5.509438514709473
```

## Image Searching

In the final part of our report, we present the image search engine with data from Duokan, an online ebook bookstore.

### Why Duokan

The website of Duokan is rich in images. Most of them are covers from various ebooks. These images are statically embeded into the HTML source, rather than dynamically loaded by JavaScript, making it possible for crawling. Description texts for images are available as `alt` attribute of `<img>`, significantly reducing parsing efforts.

### Modifying the Crawler

As the index file grows, a simple text file `index.txt` on disk is insufficient for our purpose. We switch to SQLite, a single-file database and use the `sqlite3` module to interact with it. The following code snippets present an overview of SQLite in our crawler.

Connecting the database:

```python
IMAGES_INDEX_PATH = 'index/images/image_index.sqlite'
# Disable checking for multiple threads sharing one connection as we try to
# synchronize writes with the variable lock.
db = sqlite3.connect(IMAGES_INDEX_PATH, check_same_thread=False)
main_cursor = db.cursor()

main_cursor.execute('CREATE TABLE IF NOT EXISTS indices ('
                    'url TEXT NOT NULL,'
                    'description TEXT,'
                    'title TEXT,'
                    'updated_at INTEGER NOT NULL)')
```

Insert entries into the table:

```python
self.filter.set(self.doc.url)
# find all <img> with src and alt attributes
for img in soup.find_all('img', src=True, alt=True):
    if not self.filter.query(img['src']):
        # add image URL to the Bloom filter
        self.filter.set(img['src'])
        cls.pages_count += 1
        print(self.name + ':', cls.pages_count, img['src'])
        # insert the image URL, its description and the
        # title of the webpage that links to the image
        # into the database
        self.cursor.execute(
            'INSERT INTO indices VALUES (?, ?, ?, ?)',
            (img['src'], img['alt'], title, int(time()))
        )
db.commit()
```

We crawl 40 000 images with the modified crawler. The SQLite database is available as `index/images/image_index.sqlite`.

### Indexing and Searching

`IndexFiles.py` and `SearchFiles.py` are also slightly modified. This code snippet is for retrieving rows from the database to build an index:

```python
db = sqlite3.connect('index/images/image_index.sqlite')
cursor = db.cursor()
for row in cursor.execute('SELECT * FROM indices'):
    url, description, title = row[:3]
    print('Adding ' + description)
    doc = Document()

    doc.add(Field('description',
                  ' '.join(jieba.cut_for_search(description)),
                  title_fieldtype))
    doc.add(Field('title', ' '.join(jieba.cut_for_search(title)), title_fieldtype))
    doc.add(Field('url', url, filename_fieldtype))
    writer.addDocument(doc)
```

`SearchFiles.py` takes a more direct approach:

```python
print('\nSearching for:', command)
query = QueryParser('description', analyzer).parse(' '.join(jieba.cut(command)))
scoreDocs = searcher.search(query, 20).scoreDocs

print(len(scoreDocs), 'total matching documents.')
for i, scoreDoc in enumerate(scoreDocs):
    doc = searcher.doc(scoreDoc.doc)
    for n in field_names:
        print(n + ':', doc.get(n))
    print('score:', scoreDoc.score, end='\n\n')
```

### Results

Here is an example output showing the top two hits.

```
Searching for: 计算机
20 total matching documents.
url: http://cover.read.duokan.com/mfsv2/download/fdsc3/p01sMiFnYv0F/DnRG2Ch8NH2WEa.jpg!l
description: 穿越 计算 算机 计算机 的 迷雾
title: 硅谷 之火 ： 人 与 计算 算机 计算机 的 未来 【 下载   在线 阅读   书评 】
score: 6.150972843170166

url: http://cover.read.duokan.com/mfsv2/download/fdsc3/p01qaDmym8N6/su9jOYIkFCZ7vb.jpg!l
description: 计算 算机 计算机 是 怎样 跑 起来 的
title: 计算 算机 科学 计算机 计算机科学 理论 电子 子书 电子书 【 下载   推荐   排行   书评 】   -   多 看 阅读
score: 5.373783111572266
```

## Conclusion

Combining Requests, Beautiful Soup and Lucene, we are able to built a search engine that lives entirely in command line.
